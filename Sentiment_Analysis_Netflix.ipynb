{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_Netflix",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal152k/Vishal152k.github.io/blob/master/Sentiment_Analysis_Netflix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9jGsticPOD2",
        "colab_type": "text"
      },
      "source": [
        "## Install and Import necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_cHOY2V7gxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install psycopg2-binary\n",
        "!pip install praw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAGg61uc7cZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import \n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Praw\n",
        "import praw\n",
        "import datetime\n",
        "import sqlalchemy\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy import Column, Integer, String, DateTime, Float\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "\n",
        "# Tweepy\n",
        "import tweepy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztcnDAf3vzNJ",
        "colab_type": "text"
      },
      "source": [
        "##Sentiment Analysis Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qZoEgL1SWag",
        "colab_type": "text"
      },
      "source": [
        "Define Network Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ecnCn64wpsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NETWORK PARAMETERS\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S+|[^A-Za-z0-9]+\"\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QRLCl9WSa4k",
        "colab_type": "text"
      },
      "source": [
        "Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flj2t1aywqEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_sentiment(label):\n",
        "  decode_map = {0: \"NEGATIVE\", 4: \"POSITIVE\"}\n",
        "  return decode_map[int(label)]\n",
        "\n",
        "def clean(text, stem=False):\n",
        "  text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "  tokens = []\n",
        "  for token in text.split():\n",
        "    tokens.append(token)\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "def preprocess_data():\n",
        "  train = pd.read_csv('training.1600000.processed.noemoticon.csv',\n",
        "                    encoding=\"ISO-8859-1\",\n",
        "                    names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "                    )\n",
        "  train = train.iloc[:,[0,5]]\n",
        "  train.target = train.target.apply(lambda x:map_sentiment(x))\n",
        "  train.text = train.text.apply(lambda x: clean(x))\n",
        "  df_train, df_test = train_test_split(train, test_size=0.2, random_state=42)\n",
        "  return df_train,df_test\n",
        "\n",
        "def tokenize(df_train):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(df_train.text)\n",
        "  vocab_size = len(tokenizer.word_index)+1\n",
        "  return tokenizer,vocab_size\n",
        "\n",
        "def data2array(tokenizer):\n",
        "  x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text),\n",
        "                          maxlen=SEQUENCE_LENGTH)\n",
        "  x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text),\n",
        "                        maxlen=SEQUENCE_LENGTH)\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(df_train.target.tolist())\n",
        "  y_train = encoder.transform(df_train.target.tolist())\n",
        "  y_test = encoder.transform(df_test.target.tolist())\n",
        "  y_train = y_train.reshape(-1,1)\n",
        "  y_test = y_test.reshape(-1,1)\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "def get_callbacks(path):\n",
        "  callback1 = ReduceLROnPlateau(monitor='val_loss',patience=3, cooldown=0)\n",
        "  callback2 = EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)\n",
        "  callback3 = ModelCheckpoint(path,\n",
        "                              monitor='val_loss',save_best_only=True)\n",
        "  callbacks = [callback1,callback2,callback3]\n",
        "  return callbacks\n",
        "\n",
        "def score(model,x_test,y_test):\n",
        "  score = model.evaluate(x_test,y_test,batch_size=BATCH_SIZE)\n",
        "  print()\n",
        "  print(\"ACCURACY:\",score[1])\n",
        "  print(\"LOSS:\",score[0])\n",
        "\n",
        "def plot_history(history):  #plot the trend in accuracy and loss\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(len(acc))\n",
        "  plt.plot(epochs,acc, 'b', label='Training accuracy')\n",
        "  plt.plot(epochs,val_acc, 'r', label='Validation accuracy')\n",
        "  plt.title('Training and Validation accuracy')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.plot(epochs,loss, 'b', label='Training loss')\n",
        "  plt.plot(epochs,val_loss, 'r', label='Validation loss')\n",
        "  plt.title('Training and Validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def decode_sentiment(score, include_neutral=True):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE\n",
        "\n",
        "def predict(text, include_neutral=True):\n",
        "  start_at = time.time()\n",
        "  text = clean(text)\n",
        "  x_test = pad_sequences(tokenizer.texts_to_sequences([text]),\n",
        "                         maxlen=SEQUENCE_LENGTH)\n",
        "  score = model.predict([x_test])[0]\n",
        "  label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "  return {\"label\":label, \"score\": float(score),\n",
        "          \"elapsed_time\": time.time()-start_at}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSLNiYHoSfUT",
        "colab_type": "text"
      },
      "source": [
        "Load Training data and Preprocess it in the form which we can feed into our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p98YmiBawqew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip training_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ5bZGQTymzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train, df_test = preprocess_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_wIyD6DynEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer,vocab_size = tokenize(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agP0cqMvynMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test = data2array(tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzc9X_pHSqjs",
        "colab_type": "text"
      },
      "source": [
        "Load our pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvB14exUywAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.load_model('model_weights.h5')\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Sk8gtES073",
        "colab_type": "text"
      },
      "source": [
        "Train a new Model and provide path where you want model weights to be saved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VdxNCfPQoJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_weights = 'model_weights_2.h5'\n",
        "callbacks = get_callbacks(save_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbqpixmkywRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train,y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_split= 0.1,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o68Otuz2SxVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQBpI4ELy5mJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score(model,x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdZNHqDIy524",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdC59T31NMz6",
        "colab_type": "text"
      },
      "source": [
        "##LIVE DEMONSTRATIO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TYnvhoZy-63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalised_predict_text(\"There's only sad news\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfGmiqWxK65d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "665c00e1-8fa7-4dfb-a571-56832b14ec7b"
      },
      "source": [
        "normalised_predct_text(\"Our team will bring some good news to you :)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IeVGctjv7EP",
        "colab_type": "text"
      },
      "source": [
        "##Genre-wise Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn8R7u3xUC5i",
        "colab_type": "text"
      },
      "source": [
        "Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKrFTrUF8Z43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tweets(search,number,place):\n",
        "  tweets = tweepy.Cursor(api.search,q=search+'--place:%s'%place,lang='en').items(number)\n",
        "  return tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbHPscO5UGz2",
        "colab_type": "text"
      },
      "source": [
        "Authenticate twitter api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjaEgTzL5JQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth = tweepy.OAuthHandler('','')\n",
        "auth.set_access_token('','')\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoGjuhgf2kkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "countries = pd.read_excel('countries_with_place_ids.xlsx',dtype='str')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyTPK-QEUPei",
        "colab_type": "text"
      },
      "source": [
        "Collect tweets for movies in a specific genre for all countries mentioned in our Excel file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYeVCUzr2lWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sci_fi = pd.read_excel('SciFiMovieList.xlsx')\n",
        "genre = 'Sci-Fi'\n",
        "movie_list = [i for i in sci_fi.iloc[:,0]]\n",
        "number = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JqzuTCW2mgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet_dict = []\n",
        "for movie in movie_list:\n",
        "  for i in range(len(countries)):\n",
        "    tweets = get_tweets(movie,number,countries.place_id[i])\n",
        "    tweet_dict.append({\n",
        "        'movie':movie,\n",
        "        'country':countries.Country[i],\n",
        "        'tweets':tweets   \n",
        "    }\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUpY5xZH2mFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_genre = []\n",
        "country = []\n",
        "tweet_content = []\n",
        "creation_time = []\n",
        "user_age = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLgZMS1v5cvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = time.time()\n",
        "while(i<len(tweet_dict)):\n",
        "  tweets = tweet_dict[i]['tweets']\n",
        "  for tweet in tweets:\n",
        "    tweet_genre.append(genre)\n",
        "    tweet_movie.append(tweet_dict[i]['movie'])\n",
        "\n",
        "    tweet_country.append(tweet_dict[i]['country'])\n",
        "    tweet_text.append(tweet.text)\n",
        "    tweet_user_age.append(tweet.user.created_at)\n",
        "  print(len(tweet_dict)-i-1,'left')\n",
        "  i+=1\n",
        "print(time.time()-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOTH5rnLVCIO",
        "colab_type": "text"
      },
      "source": [
        "Add those collected tweets and determine their sentiments. This data is then added to a pandas dataframe and saved in an Excel spreadsheet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omXO6TKs5iP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genre_data = pd.DataFrame()\n",
        "genre_data['Genre'] = pd.Series(tweet_genre)\n",
        "genre_data['Movie'] = pd.Series(tweet_movie)\n",
        "genre_data['Country'] = pd.Series(tweet_country)\n",
        "genre_data['Tweet'] = pd.Series(tweet_text)\n",
        "genre_data['TwitterAge'] = pd.Series(tweet_user_age)\n",
        "\n",
        "genre_data['Sentiment'] = genre_data.Tweet.apply(lambda x:predict(x)['label'])\n",
        "genre_data['SentimentScore'] = genre_data.Tweet.apply(lambda x:predict(x)['score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gLriSLk8GNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genre_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVd6vTfF5kr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genre_data.to_excel('SciFi_data.xlsx',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}